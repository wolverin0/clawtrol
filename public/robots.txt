# See https://www.robotstxt.org/robotstxt.html for documentation on how to use the robots.txt file

# Allow all search engines to crawl the site
User-agent: *
Allow: /

# Disallow private/authenticated sections
Disallow: /projects/
Disallow: /tasks/

# Sitemap location (update when sitemap is created)
Sitemap: https://cardy.so/sitemap.xml
